{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Decoder GPT - \n",
    "\n",
    "Week 6 of MLX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (0.17.2)\n",
      "Requirement already satisfied: filelock in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: numpy in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: opencv-python in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: pip in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (23.3.1)\n",
      "Requirement already satisfied: install in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (1.3.5)\n",
      "Requirement already satisfied: pandas in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: datasets in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: requests in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (2.31.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from datasets) (3.13.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: packaging in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.21 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danielhall/miniconda3/envs/encoderdecodergpt/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install torch torchvision\n",
    "!pip install opencv-python\n",
    "!pip install pip install pandas datasets requests\n",
    "!pip install matplotlib\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import requests\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on large scale: False, Running on cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x114d0a750>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration\n",
    "\n",
    "using_sentence_piece = True\n",
    "\n",
    "\n",
    "# Sentence piece config\n",
    "num_entries_for_sentence_piece_model = 12000\n",
    "sentencepiece_output_dir = '../sentencepiece_models'\n",
    "sentencepiece_corpus_filename = f\"image_captions.txt\"\n",
    "sentencepiece_model_prefix = os.path.join(sentencepiece_output_dir, 'image_captions_spm')\n",
    "caption_vocabulary_size = 10000\n",
    "\n",
    "# ---------------\n",
    "# Old configuration below\n",
    "\n",
    "should_train_large_scale = False\n",
    "\n",
    "if should_train_large_scale:\n",
    "    batch_size = 64 # How many independent sequences to process in parallel\n",
    "    block_size = 256 # The maximum context length for predictions\n",
    "    num_heads = 6 # The number of attention heads\n",
    "    max_epochs = 5000 # The maximum number of epochs to train for\n",
    "    eval_interval = 500\n",
    "    learning_rate = 3e-4\n",
    "    eval_iters = 200\n",
    "    num_embedding_dimensions = 384\n",
    "    num_layers = 6\n",
    "    dropout = 0.2\n",
    "else:\n",
    "    batch_size = 32 # How many independent sequences to process in parallel\n",
    "    block_size = 8 # The maximum context length for predictions\n",
    "    num_heads = 4 # The number of attention heads\n",
    "    max_epochs = 5000 # The maximum number of epochs to train for\n",
    "    eval_interval = 300\n",
    "    learning_rate = 1e-3\n",
    "    eval_iters = 200\n",
    "    num_embedding_dimensions = 32\n",
    "    num_layers = 3\n",
    "    dropout = 0.1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on large scale: {should_train_large_scale}, Running on {device}\")\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[168], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlphuji/flickr30k\u001b[39m\u001b[38;5;124m\"\u001b[39m, streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m dataset_stream \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m first_entry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset_stream\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m image \u001b[38;5;241m=\u001b[39m first_entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(image)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "# Load the Flickr30k dataset from Hugging Face\n",
    "dataset = load_dataset(\"nlphuji/flickr30k\", streaming=True)\n",
    "\n",
    "dataset_stream = dataset[\"test\"]\n",
    "\n",
    "first_entry = next(iter(dataset_stream))\n",
    "\n",
    "image = first_entry[\"image\"]\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Hide axes to focus on the image\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Data processing helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out data into training and validation\n",
    "def split_data_train_validation(data):\n",
    "    n = int(0.9*len(data))\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "\n",
    "    print(f\"len(train_data): {len(train_data)}, len(val_data): {len(val_data)}\")\n",
    "\n",
    "    assert len(train_data) == int(0.9*len(data))\n",
    "    assert len(val_data) == len(data) - len(train_data)\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X - Sentence piece setup\n",
    "\n",
    "Generate the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "# Specify the directory where you want to save the files\n",
    "if not os.path.exists(sentencepiece_output_dir):\n",
    "    os.makedirs(sentencepiece_output_dir)\n",
    "\n",
    "# Save all texts to a single file in the specified directory, one story per line\n",
    "sentencepiece_corpus_file_path = os.path.join(sentencepiece_output_dir, sentencepiece_corpus_filename)\n",
    "\n",
    "# Initialize a list to store the data\n",
    "captions = []\n",
    "\n",
    "# Stream data and collect only the 'caption' column\n",
    "# Save all texts to a single file, one story per line\n",
    "with open(sentencepiece_corpus_file_path, 'w', encoding='utf-8') as f:       \n",
    "    sample_count = 0\n",
    "    for sample in dataset_stream:\n",
    "        sample_count += 1\n",
    "        for caption in sample['caption']:\n",
    "            f.write(caption + '\\n')\n",
    "            captions.append(caption)\n",
    "        if sample_count >= num_entries_for_sentence_piece_model:  # Stop after collecting 10 entries\n",
    "            break\n",
    "\n",
    "# Convert the list to a pandas DataFrame\n",
    "df_captions = pd.DataFrame(captions, columns=['caption'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(len(df_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../sentencepiece_models/image_captions.txt\n",
      "  input_format: \n",
      "  model_prefix: ../sentencepiece_models/image_captions_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ../sentencepiece_models/image_captions.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 60000 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=3713418\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9507% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=51\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999507\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 60000 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=2105985\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 32998 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 60000\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 20525\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 20525 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=13223 obj=8.91689 num_tokens=40343 num_tokens/piece=3.05097\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10530 obj=6.85579 num_tokens=40393 num_tokens/piece=3.83599\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: ../sentencepiece_models/image_captions_spm.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: ../sentencepiece_models/image_captions_spm.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(input=sentencepiece_corpus_file_path, model_prefix=sentencepiece_model_prefix, vocab_size=caption_vocabulary_size, character_coverage=0.9995, model_type='unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm_model_path = f\"{sentencepiece_model_prefix}.model\"\n",
    "sp = spm.SentencePieceProcessor(model_file=spm_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1477, 1640, 583]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Attempt the encodings\n",
    "\n",
    "test_string = \"hii there\"\n",
    "\n",
    "tokens = sp.encode(test_string, out_type=int)\n",
    "\n",
    "print(tokens)\n",
    "print(sp.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of text: 3713418\n",
      "data.shape: torch.Size([831076]), data.dtype: torch.int64\n",
      "tensor([  21,   29,  396,   13, 1888,  139,  200,   22,    7,   76,  183,   33,\n",
      "         366,   83,    6,    7,  418,    4,   21,   29,   14,  922,  797,   18,\n",
      "          66,   88,   11,   92, 1534,    4,   21,   40,    6,   58,  311,   18,\n",
      "          39,    6,    3,  418,    4,    5,   11,    6,    3,   31,   27,   39,\n",
      "           6,    3,  903,    4,   21,  557,  538,  687, 3356, 2048,  141,    4,\n",
      "         290,  279,   40,    6,  322,  294,   18, 1564,    3,  889, 2670, 2936,\n",
      "           4,  852,  200,   46,   77,   57,  374,    8,    3,  306,   12,  465,\n",
      "           4,   21,   40,  114,    8,    3,  443,   23,  322,  294,    4,  153,\n",
      "          40,    8,  113,   12,    3,  448,  509,    4,   64,   42,   40,    8,\n",
      "           3,   61, 4533,    4,    5,   50,    6,    3,  104,  143,   10,  207,\n",
      "          57,    3,  407,   12,  497,    6,   34, 4916,  782,    4,    5,   55,\n",
      "          37,    6,    3,  104,  143,  285,   16,   81,    3,  217, 3168,    4,\n",
      "           5,   55,   37,  207,    7,  497,   20,   49, 4152,    4,    5,   55,\n",
      "          37,  207,   81,    3,  217, 4152,    4,    5,   37,  285,   16,   81,\n",
      "           3,  217,   98,    4,  309,  558,    6,    3,   31,   27,    9,   74,\n",
      "          10,   39,    8, 2923,    9,   19,  453,  258,  188,    3,  225,    4,\n",
      "           5,   11,    6,    3,   31,   27,   10,   39,    8,    3,  627,  503,\n",
      "           3,  225,    4,    5,   11,    8,    3,  627, 1739,    7,  225,   12,\n",
      "           3,  448,   98,    4,   97,    6,   31,   27,    9,  152,    8,  627,\n",
      "         503,  975,    5,   11,    8,    3,  627, 1739,    3,  225,   21,   40,\n",
      "          14,   62,    6,    3,  121,   27,   14,   62,    6,    3,   28,   27,\n",
      "          14,   39,   88,    3, 1148,    4,   21,  222,  375,    9, 5549,   95,\n",
      "          13,    7,  112,    4,   21,   40,    6,    3,  249,  375,  155,    8,\n",
      "           3, 1148,    4,   21,   40,   18,   22,    7, 1148,  367,   16,  155,\n",
      "           4,   21,   40,   18,  375,    3,  613,    4,   21,   26,    6,    7,\n",
      "         431,   18,   38,    7,  125,    9,    7,   80,   10, 2083, 1248,   22,\n",
      "         142,    4,    5,   11,    6,   58,  144,    3,  125,   33,    7,   80,\n",
      "          11, 2171,   32,   27,    4,    5,   11,   10,  624,   16,    7,  125,\n",
      "         300,  399,    4,    5,  222, 6377,   16,   57,   86,   11,  109,   15,\n",
      "         202,    4,   47,   70,  131,   38,  125,    5,   11,  103,    6,    3,\n",
      "         220,   33,   45,    3,   61,  760,   56,  618,   12,    3, 2798,    4,\n",
      "           5,   11,   10,   36,    8,    3,  220,   45,    3,   61,  760,   56,\n",
      "         618,    4,    5,   11, 2498,   15,    7, 1693,   16, 2710,   15,    8,\n",
      "           3,  760,   56, 2798,    4,    5,   11,  144,    3,   61,  760,   56,\n",
      "        2798,  180,    4,    5,   11,   10,  135,   22,    3,  760,   56, 2798,\n",
      "           5,   37,   10,    8, 2564,  129,    8,   49,  365,   39,    6,    3,\n",
      "         696,  352,    4,    5, 2081, 5997,   37,  129,    8,   49,  365,   33,\n",
      "        3963,   16, 4481,   46,    7,   52,    4,    5,   29,  446,   23, 3230,\n",
      "          14,   45,    3, 4367, 2701,  463,   20,   49, 1027,    4,  216,   10,\n",
      "           3,   29,   37,    8,   49,  365,   33,  923,    4,  235,  129,    8,\n",
      "         365,    9,   23, 2564,    4,   73,  517,   11,   23,    3,   28,  193,\n",
      "          96,   88,    3,  161,   41,  210,   17,    9,    3,   53,   41,  210,\n",
      "          17,    4,   64,   42,   26,   18,   39,   66,   88,   61, 2249,    9,\n",
      "           3,  327,  746,    4,    5,   29,   17,  149,  316,   70,   29,   26,\n",
      "          94,    6, 3213,   28,  874,    4,    5,   17,   13,    3,   61,  913,\n",
      "          10,   51,   54,    3, 1152,    4,  290,  279,   26,   39,   66,    3,\n",
      "          98,    4,   21,   40,    6, 1151,   92,   89,   78,    3,  712,   22,\n",
      "           7,  707,  687,   13,  807,  311,    4,   21, 1422,   15,   18,   89,\n",
      "          78,    3, 2922,  746,   14,   22,  320,    4, 1417,  333,    8, 1131,\n",
      "           6,    7,  215,   12,    7,  320,    4,   21,   40,   13,  572,  311,\n",
      "          89,   78,    3,  712,    4,   21,  396,   89,   78,    3, 1152,  141,\n",
      "         340, 2319, 1663, 1287, 1142,  335,    6,    3,  333, 1563,   13, 2056,\n",
      "         740,   16,   60,    3,  225,    4, 5903, 2072, 1663,    6,    3, 1563,\n",
      "        1019,   89,   13, 3854, 2251, 1543,    4,  340,  111,   18,  733, 3383,\n",
      "           6,    3,  493, 1019,  186,    4,  340,  111,  333,    9, 1172,  585,\n",
      "           6, 2319,  841,    4,    5, 2319,  841,   12,  884,  111,   89,    6,\n",
      "        7480,    4,   64,   42,   29,   40,    9,    3,   29,   17,   23, 1847,\n",
      "          18,  733,    6,  579,   22,    7,  113,   12,    3, 1416,   12,  433,\n",
      "         497,    4,  153, 2155,   94,  396,   89,   46,    3, 2054,  326,   13,\n",
      "           3,  351,  120,  105,  170,    4,  153,  396,  229,   23,  294,   62,\n",
      "        1200,   18,   89,   22,    7,  113,   12,    3, 1133,    4,  153,   40,\n",
      "          13, 1580,  786,   18,   89,   46,   77,    7,  113,   12,  497,    4,\n",
      "         153,   26,   18,   89,   77,    7,  113,   12,    3, 1416,   12,  497,\n",
      "           4,    5,   28,   25,    9,    3,   24,   25,   13,   53, 2167,   18,\n",
      "         655,   22,  178,   80,    6,    7,   52,    4,    5,   28,   25,    9,\n",
      "           3, 2530,   41, 1257,   25,   38,   13,  178,   80,    8,    7,  172,\n",
      "           4,   21,   91,   12,  667, 3272,   15,   65,   22,  178,   80,    8,\n",
      "           7,  172,    4,   21,   91,    8,  983,  958,  214,  178,   80,    4,\n",
      "           5,   28,   25,    9,    3, 3323,   56,   25,   18,  737,    5,   11,\n",
      "          13, 1171,  672,  485,  373,    9, 1027, 2300, 1016,    3,   19,    0,\n",
      "        2873, 3130,   42, 1092,    8,    3,  172,    4,   19,    0, 2873, 3130,\n",
      "          42, 1092,   15, 3649,   15,   46,    3,   52,   14,   33,    7, 1095,\n",
      "         455,   15, 6847,   92,   20, 1000,  270,    4,    5,   11,    6,    3,\n",
      "        1209,   58,    9,   84,  239,   10,  511,    8,    3,   58, 1092,    4,\n",
      "           5,   11,    6,    3, 1092,   23,  849,  511,   46,    3, 1083,   52,\n",
      "           4,    5,   11,  511,    3,   19,    0, 2873, 3130,   42, 1092,    8,\n",
      "           3, 3008,  172,    6,    7,  928,    4,  309,   67,   18,   39,    6,\n",
      "          44,   12,    3,  350,   13,  763,  105,  150,    4,  290,  279,   67,\n",
      "         116,    8,    3,  169,   52,   13,  448,  763,    4,    5,   43,   12,\n",
      "          67,   18,   39,    6,   44,   12,    3,  350,    4,  290,  279,   67,\n",
      "         752,   66,    6,    3,  169,    4,  551,   18,   39,   66,    5,   29,\n",
      "          17,   13,  161,  139,    9,   23,  157,   10,  537,   16,   24, 2845,\n",
      "           8,    3,  541,  259,   16,    3, 3727,  354,    4,    5,  164,    6,\n",
      "           3,   28,  113,   13])\n",
      "len(train_data): 747968, len(val_data): 83108\n"
     ]
    }
   ],
   "source": [
    "with open('../datasets/image_captions/input.txt', 'r', encoding='utf-8') as f:\n",
    "    caption_text = f.read()\n",
    "\n",
    "print (f\"Lenght of text: {len(caption_text)}\")\n",
    "\n",
    "caption_data = torch.tensor(sp.encode(caption_text), dtype=torch.long)\n",
    "print(f\"data.shape: {caption_data.shape}, data.dtype: {caption_data.dtype}\")\n",
    "print(caption_data[:1000])\n",
    "\n",
    "caption_train_data, caption_val_data = split_data_train_validation(caption_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sidestep - character based corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of text: 3713418\n",
      "\n",
      " !\"#$&'()+,-.0123456789:;?@ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "with open('../datasets/image_captions/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print (f\"Lenght of text: {len(text)}\")\n",
    "\n",
    "# Generate tokens (on a character level)\n",
    "chars = sorted(list(set(text)))\n",
    "character_vocab_size = len(chars)\n",
    "\n",
    "print(''.join(chars))\n",
    "print(character_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 62, 62, 1, 73, 61, 58, 71, 58]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Create mappings between characters and integer tokens\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode_character = lambda s: [stoi[c] for c in s] # Convert a string to a list of integers\n",
    "decode_character = lambda l: ''.join([itos[i] for i in l]) # Convert a list of integers to a string\n",
    "\n",
    "print(encode_character(\"hii there\"))\n",
    "print(decode_character(encode_character(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: torch.Size([3713418]), data.dtype: torch.int64\n",
      "tensor([47, 76, 68,  1, 78, 68, 74, 67, 60,  1, 60, 74, 78, 72,  1, 76, 62, 73,\n",
      "        61,  1, 72, 61, 54, 60, 60, 78,  1, 61, 54, 62, 71,  1, 65, 68, 68, 64,\n",
      "         1, 54, 73,  1, 73, 61, 58, 62, 71,  1, 61, 54, 67, 57, 72,  1, 76, 61,\n",
      "        62, 65, 58,  1, 61, 54, 67, 60, 62, 67, 60,  1, 68, 74, 73,  1, 62, 67,\n",
      "         1, 73, 61, 58,  1, 78, 54, 71, 57, 13,  0, 47, 76, 68,  1, 78, 68, 74,\n",
      "        67, 60, 11,  1, 50, 61, 62, 73, 58,  1, 66, 54, 65, 58, 72,  1, 54, 71,\n",
      "        58,  1, 68, 74, 73, 72, 62, 57, 58,  1, 67, 58, 54, 71,  1, 66, 54, 67,\n",
      "        78,  1, 55, 74, 72, 61, 58, 72, 13,  0, 47, 76, 68,  1, 66, 58, 67,  1,\n",
      "        62, 67,  1, 60, 71, 58, 58, 67,  1, 72, 61, 62, 71, 73, 72,  1, 54, 71,\n",
      "        58,  1, 72, 73, 54, 67, 57, 62, 67, 60,  1, 62, 67,  1, 54,  1, 78, 54,\n",
      "        71, 57, 13,  0, 28,  1, 66, 54, 67,  1, 62, 67,  1, 54,  1, 55, 65, 74,\n",
      "        58,  1, 72, 61, 62, 71, 73,  1, 72, 73, 54, 67, 57, 62, 67, 60,  1, 62,\n",
      "        67,  1, 54,  1, 60, 54, 71, 57, 58, 67, 13,  0, 47, 76, 68,  1, 59, 71,\n",
      "        62, 58, 67, 57, 72,  1, 58, 67, 63, 68, 78,  1, 73, 62, 66, 58,  1, 72,\n",
      "        69, 58, 67, 73,  1, 73, 68, 60, 58, 73, 61, 58, 71, 13,  0, 46, 58, 75,\n",
      "        58, 71, 54, 65,  1, 66, 58, 67,  1, 62, 67,  1, 61, 54, 71, 57,  1, 61,\n",
      "        54, 73, 72,  1, 54, 71, 58,  1, 68, 69, 58, 71, 54, 73, 62, 67, 60,  1,\n",
      "        54,  1, 60, 62, 54, 67, 73,  1, 69, 74, 65, 65, 58, 78,  1, 72, 78, 72,\n",
      "        73, 58, 66, 13,  0, 50, 68, 71, 64, 58, 71, 72,  1, 65, 68, 68, 64,  1,\n",
      "        57, 68, 76, 67,  1, 59, 71, 68, 66,  1, 74, 69,  1, 54, 55, 68, 75, 58,\n",
      "         1, 68, 67,  1, 54,  1, 69, 62, 58, 56, 58,  1, 68, 59,  1, 58, 70, 74,\n",
      "        62, 69, 66, 58, 67, 73, 13,  0, 47, 76, 68,  1, 66, 58, 67,  1, 76, 68,\n",
      "        71, 64, 62, 67, 60,  1, 68, 67,  1, 54,  1, 66, 54, 56, 61, 62, 67, 58,\n",
      "         1, 76, 58, 54, 71, 62, 67, 60,  1, 61, 54, 71, 57,  1, 61, 54, 73, 72,\n",
      "        13,  0, 33, 68, 74, 71,  1, 66, 58, 67,  1, 68, 67,  1, 73, 68, 69,  1,\n",
      "        68, 59,  1, 54,  1, 73, 54, 65, 65,  1, 72, 73, 71, 74, 56, 73, 74, 71,\n",
      "        58, 13,  0, 47, 61, 71, 58, 58,  1, 66, 58, 67,  1, 68, 67,  1, 54,  1,\n",
      "        65, 54, 71, 60, 58,  1, 71, 62, 60, 13,  0, 28,  1, 56, 61, 62, 65, 57,\n",
      "         1, 62, 67,  1, 54,  1, 69, 62, 67, 64,  1, 57, 71, 58, 72, 72,  1, 62,\n",
      "        72,  1, 56, 65, 62, 66, 55, 62, 67, 60,  1, 74, 69,  1, 54,  1, 72, 58,\n",
      "        73,  1, 68, 59,  1, 72, 73, 54, 62, 71, 72,  1, 62, 67,  1, 54, 67,  1,\n",
      "        58, 67, 73, 71, 78,  1, 76, 54, 78, 13,  0, 28,  1, 65, 62, 73, 73, 65,\n",
      "        58,  1, 60, 62, 71, 65,  1, 62, 67,  1, 54,  1, 69, 62, 67, 64,  1, 57,\n",
      "        71, 58, 72, 72,  1, 60, 68, 62, 67, 60,  1, 62, 67, 73, 68,  1, 54,  1,\n",
      "        76, 68, 68, 57, 58, 67,  1, 56, 54, 55, 62, 67, 13,  0, 28,  1, 65, 62,\n",
      "        73, 73, 65, 58,  1, 60, 62, 71, 65,  1, 56, 65, 62, 66, 55, 62, 67, 60,\n",
      "         1, 73, 61, 58,  1, 72, 73, 54, 62, 71, 72,  1, 73, 68,  1, 61, 58, 71,\n",
      "         1, 69, 65, 54, 78, 61, 68, 74, 72, 58, 13,  0, 28,  1, 65, 62, 73, 73,\n",
      "        65, 58,  1, 60, 62, 71, 65,  1, 56, 65, 62, 66, 55, 62, 67, 60,  1, 62,\n",
      "        67, 73, 68,  1, 54,  1, 76, 68, 68, 57, 58, 67,  1, 69, 65, 54, 78, 61,\n",
      "        68, 74, 72, 58, 13,  0, 28,  1, 60, 62, 71, 65,  1, 60, 68, 62, 67, 60,\n",
      "         1, 62, 67, 73, 68,  1, 54,  1, 76, 68, 68, 57, 58, 67,  1, 55, 74, 62,\n",
      "        65, 57, 62, 67, 60, 13,  0, 46, 68, 66, 58, 68, 67, 58,  1, 62, 67,  1,\n",
      "        54,  1, 55, 65, 74, 58,  1, 72, 61, 62, 71, 73,  1, 54, 67, 57,  1, 61,\n",
      "        54, 73,  1, 62, 72,  1, 72, 73, 54, 67, 57, 62, 67, 60,  1, 68, 67,  1,\n",
      "        72, 73, 54, 62, 71,  1, 54, 67, 57,  1, 65, 58, 54, 67, 62, 67, 60,  1,\n",
      "        54, 60, 54, 62, 67, 72, 73,  1, 54,  1, 76, 62, 67, 57, 68, 76, 13,  0,\n",
      "        28,  1, 66, 54, 67,  1, 62, 67,  1, 54,  1, 55, 65, 74, 58,  1, 72, 61,\n",
      "        62, 71, 73,  1, 62, 72,  1, 72, 73, 54, 67, 57, 62, 67, 60,  1, 68, 67,\n",
      "         1, 54,  1, 65, 54, 57, 57, 58, 71,  1, 56, 65, 58, 54, 67, 62, 67, 60,\n",
      "         1, 54,  1, 76, 62, 67, 57, 68, 76, 13,  0, 28,  1, 66, 54, 67,  1, 68,\n",
      "        67,  1, 54,  1, 65, 54, 57, 57, 58, 71,  1, 56, 65, 58, 54, 67, 72,  1,\n",
      "        73, 61, 58,  1, 76, 62, 67, 57, 68, 76,  1, 68, 59,  1, 54,  1, 73, 54,\n",
      "        65, 65,  1, 55, 74, 62, 65, 57, 62, 67, 60, 13,  0, 40, 54, 67,  1, 62,\n",
      "        67,  1, 55, 65, 74, 58,  1, 72, 61, 62, 71, 73,  1, 54, 67, 57,  1, 63,\n",
      "        58, 54, 67, 72,  1, 68, 67,  1, 65, 54])\n",
      "len(train_data): 3342076, len(val_data): 371342\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode_character(text), dtype=torch.long)\n",
    "print(f\"data.shape: {data.shape}, data.dtype: {data.dtype}\")\n",
    "print(data[:1000])\n",
    "\n",
    "character_train_data, character_val_data = split_data_train_validation(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_token (token):\n",
    "    if using_sentence_piece:\n",
    "        return sp.encode(token, out_type=int)\n",
    "    else:\n",
    "        return encode_character(token)\n",
    "\n",
    "def decode_token (token):\n",
    "    if using_sentence_piece:\n",
    "        return sp.decode(token)\n",
    "    else:\n",
    "        return decode_character(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_sentence_piece:\n",
    "    train_data = caption_train_data\n",
    "    val_data = caption_val_data\n",
    "    vocab_size = caption_vocabulary_size\n",
    "else:\n",
    "    train_data = character_train_data\n",
    "    val_data = character_val_data\n",
    "    vocab_size = character_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, train_data, val_data):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(train_data, val_data):\n",
    "    out = {}\n",
    "    bigram_lm.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, train_data, val_data)\n",
    "            logits, loss = bigram_lm(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    bigram_lm.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BatchNorm1d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta]\n\u001b[1;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m1337\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mBatchNorm1d\u001b[49m(\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m module(x)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BatchNorm1d' is not defined"
     ]
    }
   ],
   "source": [
    "# custom implementation for reference.\n",
    "class LayerNorm1d:\n",
    "\n",
    "    def __init__(self, dimensions, epsilon=1e-5):\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = torch.ones(dimensions)\n",
    "        self.beta = torch.zeros(dimensions)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        xmean = x.mean(1, keepdim=True) # batch mean\n",
    "        xvar = x.var(1, keepdim=True) # batch variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.epsilon) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta # scale and shift\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = BatchNorm1d(100)\n",
    "x = torch.randn(32, 100)\n",
    "x = module(x)\n",
    "x.shape\n",
    "\n",
    "x[:,0].mean(), x[:,0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" One head self attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(num_embedding_dimensions, head_size, bias=False)\n",
    "        self.query = nn.Linear(num_embedding_dimensions, head_size, bias=False)\n",
    "        self.value = nn.Linear(num_embedding_dimensions, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        weigths = q @ k.transpose(-2, -1) * C ** -0.5 # (B, T, C) @ (B, C, T) = (B, T, T)\n",
    "        weigths = weigths.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weigths = F.softmax(weigths, dim=-1) # (B, T, T)\n",
    "        weigths = self.dropout(weigths)\n",
    "\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x)\n",
    "        out = weigths @ v # (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention model\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(num_heads * head_size, num_embedding_dimensions)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.projection(out)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" A simple feed-forward network \"\"\"\n",
    "\n",
    "    def __init__(self, num_embeddings):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_embeddings, 4 * num_embeddings),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * num_embeddings, num_embeddings),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, num_embeddings, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        head_size = num_embeddings // num_heads\n",
    "        self.self_attention = MultiHeadAttention(num_heads, head_size)\n",
    "        self.feed_forward = FeedForward(num_embeddings)\n",
    "        self.layer_norm_1 = nn.LayerNorm(num_embeddings)\n",
    "        self.layer_norm_2 = nn.LayerNorm(num_embeddings)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attention(self.layer_norm_1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, num_embedding_dimensions)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, num_embedding_dimensions)\n",
    "        self.blocks = nn.Sequential(*[Block(num_embedding_dimensions, num_heads) for _ in range(num_layers)])\n",
    "        self.layer_norm_final = nn.LayerNorm(num_embedding_dimensions)\n",
    "        self.lm_head = nn.Linear(num_embedding_dimensions, vocab_size)\n",
    "        \n",
    "    def forward(self, index, targets = None):\n",
    "        B, T = index.shape\n",
    "\n",
    "        # index and targets are both (Batch, Time) tensor of integers        \n",
    "        token_embeddings = self.token_embedding_table(index) # (Batch, Time, Channel)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(T, device=device)) # (Time, Channel)\n",
    "        x = token_embeddings + position_embeddings # (Batch, Time, Channel)\n",
    "        x = self.blocks(x) # (Batch, Time, Channel)\n",
    "        x = self.layer_norm_final(x) # (Batch, Time, Channel)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            index_condition = index[:, -block_size:]\n",
    "            # Get predictions\n",
    "            logits, loss = self(index_condition)\n",
    "            # Focus only on the last token (timestep)\n",
    "            logits = logits[:, -1, :] # (Batch, Vocab)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (Batch, Vocab)\n",
    "            # Sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (Batch, 1)\n",
    "            # Append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (Batch, Time + 1)\n",
    "        return index\n",
    "\n",
    "bigram_lm = BigramLanguageModel()\n",
    "m = bigram_lm.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 9.3891, Val loss: 9.3939\n",
      "Epoch: 300, Train loss: 5.2899, Val loss: 5.3517\n",
      "Epoch: 600, Train loss: 4.8757, Val loss: 4.9441\n",
      "Epoch: 900, Train loss: 4.6511, Val loss: 4.7611\n",
      "Epoch: 1200, Train loss: 4.5083, Val loss: 4.5960\n",
      "Epoch: 1500, Train loss: 4.4399, Val loss: 4.5369\n",
      "Epoch: 1800, Train loss: 4.3196, Val loss: 4.4279\n",
      "Epoch: 2100, Train loss: 4.2578, Val loss: 4.3899\n",
      "Epoch: 2400, Train loss: 4.2069, Val loss: 4.3207\n",
      "Epoch: 2700, Train loss: 4.1552, Val loss: 4.3055\n",
      "Epoch: 3000, Train loss: 4.1474, Val loss: 4.2578\n",
      "Epoch: 3300, Train loss: 4.0811, Val loss: 4.2359\n",
      "Epoch: 3600, Train loss: 4.0440, Val loss: 4.2054\n",
      "Epoch: 3900, Train loss: 3.9938, Val loss: 4.1952\n",
      "Epoch: 4200, Train loss: 4.0130, Val loss: 4.1437\n",
      "Epoch: 4500, Train loss: 3.9874, Val loss: 4.1723\n",
      "Epoch: 4800, Train loss: 3.9594, Val loss: 4.1278\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(bigram_lm.parameters(), lr=learning_rate)\n",
    "\n",
    "batch_size = 32\n",
    "for iter in range(max_epochs):\n",
    "\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss(train_data, val_data)\n",
    "        print(f\"Epoch: {iter}, Train loss: {losses['train']:.4f}, Val loss: {losses['val']:.4f}\")\n",
    "\n",
    "    # sampel a batch of data\n",
    "    xb, yb = get_batch('train', train_data, val_data)\n",
    "\n",
    "    # evaluate the loss\n",
    "    token_embeddings, loss = bigram_lm(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚Åá  Young child looks down and reading trees in their mouth. A little girl is on the grass. A black learn-haired man in a boat is going. There is kissing in a double bracelet ball glass. A brown dog stands at night. tall-hair. A black and glasses and playing in the viewer past a child talking. A man smokesther with a green tank exameasant apart. A man with backpacks is performing near a building Small art the street with a pool. A Sooner and sleep kid kicks stuffs a toy in top. Two small children play on a how to the city laying a girl with a small child looks at a barb. A small bride is crawling on a song after a green wedding onto a high beach. A girl in a striped shirt holding a stone bike in rough above water. A man with gear, two boys and one people. Thi other male is a man stands on the bus is dressed in ancient. Woman standing near the beach. A woman touching a man pushes up a slope. A cowboy silver walks on a leash in the water A man dressed in a background. A man covereder is walk near a brown building. A se dog stirs to rooster with a building performs that strums kneels da outfit while another height is walking while a man worshipp into a concert of wooding each other in a race couchicken of the fence with long hooded leash. dealer turned snowboarders. A man is walking down his trees in front of the top. A girl in motion with his hair is taking out of a staircase. Two standing cafe. A groom in an open and gray area with something. Two people watch their skilleting sitting on the wooden public with a headam. Two elder scattered chocolate people are laughing performers. A little girl in him. A man carries a tree polished something stands. A man standing around a ramp and a camcorder in match. where him rainbow sweeping bagss of a straw table on a loom next to line. A woman wearing a black top looks card a ball with a bicycle of the ball surrounded shirt and an adult canopy even under concrete near a sunset. A woman where objects share of her eyes laughing. A yellow dog, is walking across an audience. The football player rides near a fence. A man in an orange shirt sits outdoors and a man in a white suit hat has the sun textile of meat on a swing. The dog is playing with a- anticipati and a\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "print(decode_token(bigram_lm.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337) \n",
    "B,T,C  = 4, 8, 2 # Batch size, Time, Channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape\n",
    "\n",
    "# bow = bag of words\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1,:] # (t, C)\n",
    "        xbow[b,t] = torch.mean(xprev, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2\n",
    "weigths = torch.tril(torch.ones(T, T))\n",
    "weigths = weigths / weigths.sum(1, keepdim=True)\n",
    "weigths\n",
    "xbow2 = weigths @ x # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "weigths = torch.zeros((T,T))\n",
    "weigths = weigths.masked_fill(tril == 0, float('-inf'))\n",
    "weigths = F.softmax(weigths, dim=1)\n",
    "xbow3 = weigths @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 4: seld-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 32 # Batch, Time, Channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Let's see a single head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "\n",
    "weigths = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# weigths = torch.zeros((T,T))\n",
    "weigths = weigths.masked_fill(tril == 0, float('-inf'))\n",
    "weigths = F.softmax(weigths, dim=-1)\n",
    "out = weigths @ x\n",
    "\n",
    "v = value(x)\n",
    "out = weigths @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tril stands for triangle lower\n",
    "torch.tril(torch.ones(3,3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor([[0.1667, 0.3333, 0.5000],\n",
      "        [0.2667, 0.3333, 0.4000]])\n",
      "--\n",
      "b= tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c= tensor([[5.3333, 5.0000],\n",
      "        [4.9333, 5.2000]])\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "#a  = torch.ones(3,3)\n",
    "#a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "b = torch.randint(0,10, (3,2)).float()\n",
    "c = a @ b\n",
    "print(f\"a= {a}\")\n",
    "print('--')\n",
    "print(f\"b= {b}\")\n",
    "print('--')\n",
    "print(f\"c= {c}\")\n",
    "print('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of why we need scaled attention\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9331)\n",
      "tensor(0.8879)\n",
      "tensor(0.8150)\n"
     ]
    }
   ],
   "source": [
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim= -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of Broadcasting in pytorch\n",
    "\n",
    "Example without broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 4],\n",
      "        [6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor of shape (2, 3) - imagine this as two sets of 3-channel embeddings\n",
    "a = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]])\n",
    "\n",
    "# Creating another tensor of shape (2, 3) - similar structure as 'a'\n",
    "b = torch.tensor([[1, 1, 1], \n",
    "                  [2, 2, 2]])\n",
    "\n",
    "# Direct addition, no broadcasting needed as shapes are identical\n",
    "result = a + b\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 4],\n",
      "        [5, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor of shape (2, 3) - imagine this as two sets of 3-channel embeddings\n",
    "a = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]])\n",
    "\n",
    "# Creating a tensor of shape (3,) - a single 3-channel embedding\n",
    "b = torch.tensor([1, 1, 1])\n",
    "\n",
    "# Addition with broadcasting: 'b' is automatically expanded to match the shape of 'a'\n",
    "result = a + b\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptfromscratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
